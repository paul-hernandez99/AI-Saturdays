{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice: Fashion MNIST con diversas redes\n",
    "\n",
    "### Ejemplo adaptado por el equipo de AI Saturdays Euskadi.\n",
    "\n",
    "Con este Practice el objetivo será el de entender cómo puede ser el proceso de modelado de redes neuronales con una serie de datos dados.\n",
    "\n",
    "En este caso, el dataset que se usará es el __*Fashion MNIST*__, el cual es análogo al dataset original [MNIST (creado por Yann LeCun et al.)](http://yann.lecun.com/exdb/mnist/), solo que en vez de clasificar 10 digitos (los dígitos del 0 al 9), se clasifican __prendas__.\n",
    "\n",
    "Para ello, los labels o etiquetas que nos encontraremos harán referencia a los siguientes:\n",
    "\n",
    "| Label | Descripción |\n",
    "| :-: | :- |\n",
    "| 0 | Camiseta / Top |\n",
    "| 1 | Pantalón |\n",
    "| 2 | Jersey |\n",
    "| 3 | Vestido |\n",
    "| 4 | Abrigo |\n",
    "| 5 | Sandalia |\n",
    "| 6 | Camisa |\n",
    "| 7 | Zapatilla |\n",
    "| 8 | Bolso |\n",
    "| 9 | Botín |\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "- Se usará el lenguaje de programación Python 3.\n",
    "- Se usarán las librerías de python: Pandas, Numpy y Keras.\n",
    "\n",
    "**Mediante este ejercicio, aprenderemos:**\n",
    "- Entender y ejecutar los Notebooks con Python.\n",
    "- Ser capaz de utilizar funciones de Python y librerías adicionales.\n",
    "- Aplicar un modelo de NN.\n",
    "- Mejorar la predicción optimizando el modelo.\n",
    "- Hacer comparación con otros modelos de NN, cambiando ajustes y sus arquitecturas correspondientes.\n",
    "\n",
    "¡Empecemos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Importación de librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como trabajaremos con Keras, las librerías que se importarán son relativas a Keras. ¡Recuerda instalar Keras en tu entorno en caso de que aún no lo tengas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importa el dataset.\n",
    "\n",
    "Este dataset está ya integrado dentro de Keras en su respectivo ```tensorflow.keras.datasets.fashion_mnist```. Por lo tanto, utilizaremos su método propio ```load_data()``` para cargar los Train y Test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solo una linea de codigo.\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Aplana el dataset.\n",
    "\n",
    "Al igual que en el dataset MNIST, en el Fashion MNIST tratamos ficheros de dos dimensiones (imágenes de tamaño 28x28 píxeles). Por ello, y como el modelo que vamos a generar primero se basa en el perceptrón con 1 capa, necesitamos aplanar los datos de training y test, y después convertirlos en variables categóricas.\n",
    "\n",
    "__Pistas: ```reshape()``` y ```to_categorical()``` te pueden ayudar.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuatro lineas de codigo\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], -1) / 255.0\n",
    "x_test = x_test.reshape(x_test.shape[0], -1) / 255.0\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imprime los tamanos de nuestros vectores de train y test\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo 1: Perceptrón."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin entrar demasiado al detalle de las funciones de activación, a modo de \"regla de oro\", plantéate el uso de estas activaciones en las siguientes situaciones:\n",
    "\n",
    "1. Utiliza ReLU ('relu') cuando puedas, para las neuronas de cada capa oculta.\n",
    "2. Utiliza Softmax ('softmax') cuando tu output quieres que sea una clasificación entre más de dos categorías.\n",
    "3. Utiliza Sigmoid ('sigmoid') cuando tu output conste de dos categorías.\n",
    "\n",
    "Como puedes observar a continuación, la __construcción__ de un modelo consta de las siguientes partes:\n",
    "\n",
    "* ```Sequential()```: Indica a Keras que vas a empezar a añadir una secuencia de capas.\n",
    "* ```add()```: Añades la capa con los detalles que necesitas. En la primera capa que generes has de definir la dimensión del input (```input_dim```), en las siguientes no es necesario.\n",
    "* ```compile()```: Das las pautas de cómo se ha de entrenar la red (función de loss, optimizador y métrica a optimizar).\n",
    "\n",
    "Por lo tanto, vamos a crear el primer modelo, basado en el Perceptrón, para resolver este problema de clasificación que tenemos entre manos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construccion del modelo\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=784, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora vamos a __entrenar__ el modelo. Como se puede observar, se indica el número de _epochs_ (iteraciones completas sobre el dataset) que queremos hacer, así como la división de datos de entrenamiento y validación (0.1 implica un 10%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y una vez hecho el entrenamiento, vamos a __evaluar__ la precisión en el conjunto de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluacion de precision del modelo\n",
    "\n",
    "_, test_acc = model.evaluate(x_test, y_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Modelo 1__: Hemos logrado una precisión aproximada de un __84%__... ¡A ver si podemos hacerlo mejor!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Modelo 2: Perceptrón con más neuronas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a hacer __exactamente lo mismo que para el Modelo 1__, pero en vez de tener 10 neuronas, ponle 50 en la primera capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construccion del modelo\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(50, input_dim=784, activation='relu'))\n",
    "model2.add(Dense(10, activation='softmax'))\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo\n",
    "\n",
    "model2.fit(x_train, y_train, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluacion de precision del modelo\n",
    "\n",
    "_, test_acc = model2.evaluate(x_test, y_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Modelo 2__: Hemos logrado una precisión aproximada de un __86%__... Ha mejorado, ¡pero a ver si podemos hacerlo mejor!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Modelo 3: Perceptrón Multicapa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora, vamos a añadir __una nueva capa al Perceptrón__, para convertirlo en un Perceptrón Multicapa. \n",
    "\n",
    "Se supone que con esto deberíamos conseguir un mejor output (cuanto más profunda la red, generalmente ajusta mejor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construccion del modelo\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(50, input_dim=784, activation='relu'))\n",
    "model3.add(Dense(50, activation='relu'))\n",
    "model3.add(Dense(10, activation='softmax'))\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo\n",
    "\n",
    "model3.fit(x_train, y_train, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluacion de precision del modelo\n",
    "\n",
    "_, test_acc = model3.evaluate(x_test, y_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Modelo 3__: Hemos logrado una precisión aproximada de un __87%__... Ha mejorado, pero tampoco es que sea para echar cohetes el asunto...\n",
    "\n",
    "¿Habrá que probar una __arquitectura distinta__?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Modelo 4: Red Neuronal Convolucional (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se va a entrar demasiado al detalle, pero una __Red Neuronal Convolucional (CNN)__ permite detectar patrones en imágenes mejor que una red de Perceptrón, debido al tipo de operaciones que ejecuta.\n",
    "\n",
    "Por ello, te dejamos aquí un _snippet_ de código bastante habitual de cómo se usan. Como verás, hay varios nuevos imports:\n",
    "\n",
    "* ```Conv2D```: Permite hacer operaciones de convolución.\n",
    "* ```MaxPooling2D```: Permite hacer operaciones de Pooling.\n",
    "* ```Flatten```: Facilita el proceso de Flatten o aplanado de los resultados previos.\n",
    "\n",
    "Y normalmente se suelen usar de manera encadenada, debido a cómo es la operación de convolución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargado de librerias\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "import numpy as np\n",
    "\n",
    "# Seleccion de train y test set\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train = x_train[:,:,:,np.newaxis] / 255.0\n",
    "x_test = x_test[:,:,:,np.newaxis] / 255.0\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construccion del modelo\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28, 1))) \n",
    "model4.add(MaxPooling2D(pool_size=2))\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(10, activation='softmax'))\n",
    "model4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Representacion de la arquitectura del modelo\n",
    "\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo\n",
    "\n",
    "model4.fit(x_train, y_train, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluacion de precision del modelo\n",
    "\n",
    "_, test_acc = model4.evaluate(x_test, y_test)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Anda! Ha mostrado una mejor performance que las arquitecturas anteriores..., aproximadamente de un __90%__.\n",
    "\n",
    "Aunque cabe decir que era de esperar, se ha demostrado que las __CNNs__ son buenas para el tratamiento de imágenes debido al tipo de operaciones que realizan (operación de convolución).\n",
    "\n",
    "No obstante, no está dentro del temario de este itinerario de _Machine Learning_ el tratar las bondades de estas CNNs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
