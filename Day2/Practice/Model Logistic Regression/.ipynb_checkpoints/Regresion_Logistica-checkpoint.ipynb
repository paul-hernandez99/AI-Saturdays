{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\"> 3. Regresión logística </h1>\n",
    "<br>\n",
    "La regresión logística es similar a la regresión (lineal), pero adaptada para fines de clasificación. La diferencia es pequeña; para la Regresión logística también tenemos que aplicar iterativamente el método de descenso de gradiente para estimar los valores del parámetro $ \\theta $. Y nuevamente, durante la iteración, los valores se estiman tomando el gradiente de la función de costo. Y también, la función de costo viene dada por el error al cuadrado de la diferencia entre la función de hipótesis $ h _{\\theta} (x) $ y $ Y $. Sin embargo, la principal diferencia es la función de hipótesis en sí.\n",
    "\n",
    "Para comprender la función de hipótesis de la regresión logística, primero debemos comprender la idea detrás de la clasificación.\n",
    "\n",
    "Cuando desee clasificar algo, hay un número limitado de clases a las que puede pertenecer. Y para cada una de estas posibles clases, solo puede haber dos estados para $ y ^ {(i)} $;\n",
    "o $ y ^ {(i)} $ pertenece a la clase especificada y $ y = 1 $, o no pertenece a la clase y $ y = 0 $. Aunque los valores de salida $ Y $ son binarios, las variables independientes $ X $ siguen siendo continuas. Entonces, necesitamos una función que tenga como entrada un gran conjunto de variables continuas $ X $ y para cada una de estas variables produzca una salida binaria. Esta función, la función de hipótesis, tiene la siguiente forma:\n",
    "\n",
    "$ h_{\\theta} = \\frac{1}{1 + \\exp(-z)} = \\frac{1}{1 + \\exp(-\\theta x)} $.\n",
    "\n",
    "Esta función también se conoce como función logística, que forma parte de la familia de la función sigmoidea. Estas funciones se utilizan ampliamente en las ciencias naturales porque proporcionan el modelo más simple para el crecimiento de la población. Sin embargo, la razón por la que la función logística se utiliza para la clasificación en Machine Learning es su 'forma de S'.\n",
    "\n",
    "<img src=\"img/Logistic-curve.svg_-350x233.png\"/>\n",
    "\n",
    "Como puede ver, esta función está limitada en la dirección y por 0 y 1. Si la variable $ z $ es muy negativa, la función de salida irá a cero (no pertenece a la clase). Si la variable $ z $ es muy positiva, la salida será uno y pertenece a la clase.\n",
    "(Esta función se denomina función indicadora).\n",
    "\n",
    "La pregunta entonces es, ¿qué pasará con los valores de entrada que no son ni muy positivos ni muy negativos, sino en algún lugar \"en el medio\"? Tenemos que definir un límite de decisión, que separa la clase positiva de la negativa. Por lo general, este límite de decisión se elige en el medio de la función logística, es decir, en $ z = 0 $ donde el valor de salida $ y $ es $ 0.5 $.\n",
    "\n",
    "\\begin{equation}\n",
    "y =\\begin{cases}\n",
    "1, \\text{si $z \\gt  0 $}.\\\\\n",
    "0, \\text{si $z \\lt 0 $}.\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Como podemos ver en la fórmula de la función logística, $ z = \\theta \\cdot x $. Es decir, el parámetro dependiente $ \\theta $ (también conocido como la característica), asigna la variable de entrada $ x $ a una posición en el eje $ z $. Con su valor $ z $, podemos usar la función logística para calcular el valor $ y $. Si este $ y $ -valor $ \\gt 0.5 $ asumimos que pertenece a esta clase y viceversa.\n",
    "\n",
    "Entonces, la característica $ \\theta $ debe elegirse de manera que prediga la pertenencia a la clase correctamente. Por tanto, es fundamental saber qué características son útiles para la tarea de clasificación. Una vez que se  seleccionan las funciones adecuadas, se puede utilizar el descenso de gradiente para encontrar el valor óptimo de estas funciones.\n",
    "\n",
    "¿Cómo podemos hacer un descenso de gradientes con esta función logística? Excepto por la función de hipótesis que tiene una forma diferente, el método de descenso de gradiente es exactamente el mismo. De nuevo tenemos una función de costo, de la cual tenemos que tomar iterativamente el gradientec con respecto a la característica $ \\theta $ y actualice el valor de la característica en cada iteración.\n",
    "\n",
    "Esta función de costo viene dada por la función logarítmica de verosimilitud conocida como entropía cruzada binaria:\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "J(x) = -\\frac{1}{2n} \\sum_i^n \\left(  y^{(i)} log( h_{\\theta}(x^{(i)})) + (1-y^{(i)})log(1-h_{\\theta}(x^{(i)})) \\right) \\\\\n",
    " = -\\frac{1}{2n} \\sum_i^n \\left( y^{(i)} log(\\frac{1}{1+exp(-\\theta x)}) + (1-y^{(i)})log(1-\\frac{1}{1+exp(-\\theta x)}) \\right)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Sabemos que:<br>\n",
    "$ log(\\frac{1}{1+exp(-\\theta x)}) = log(1) - log(1+exp(-\\theta x)) = - log(1+exp(-\\theta x))$\n",
    "\n",
    "y<br>\n",
    "\n",
    "$ log(1-\\frac{1}{1+exp(-\\theta x)}) = log( \\frac{exp(-\\theta x)}{1+exp(-\\theta x)}) $\n",
    "\n",
    "$ = log(exp(-\\theta x)) - log(1+exp(-\\theta x)) $\n",
    "\n",
    "$ = -\\theta x^{(i)} -  log(1+exp(-\\theta x)) $\n",
    "\n",
    "Reemplazar estas dos ecuaciones en la función de costo nos da:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "J(x)  = - \\frac{1}{2n} \\sum_i^n \\left( - y^{(i)} log(1+exp(-\\theta x)) - (1-y^{(i)})(\\theta x^{(i)} +  log(1+exp(-\\theta x))) \\right) \\\\\n",
    " = - \\frac{1}{2n} \\sum_i^n \\left(  y^{(i)} \\theta x^{(i)} -\\theta x^{(i)} -log(1+exp(-\\theta x)) \\right)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "&nbsp;\n",
    "\n",
    "El gradiente de la función de costo con respecto a $ \\theta $ está dado por\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d}{d\\theta} J(x) = - \\frac{1}{2n} \\sum_i^n \\left(  y^{(i)} x^{(i)} - x^{(i)} + x^{(i)} \\frac{ exp(-\\theta x)}{1+exp(-\\theta x)} \\right) \\\\\n",
    " = - \\frac{1}{2n} \\sum_i^n \\left( x^{(i)} ( y^{(i)} - 1 +\\frac{exp(-\\theta x)}{1+exp(-\\theta x)} ) \\right) \\\\\n",
    " = - \\frac{1}{2n} \\sum_i^n  \\left( x^{(i)} ( y^{(i)} - \\frac{1}{1+exp(-\\theta x)} ) \\right) \\\\\n",
    " = - \\frac{1}{2n} \\sum_i^n  \\left( x^{(i)} ( y^{(i)} - h_{\\theta}(x^{(i)}) )\\right)\n",
    "\\end{align}\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Entonces, el gradiente de la función de costo aparentemente difícil resulta ser una ecuación mucho más simple. Y dado que es el gradiente que usamos para actualizar los valores de $ \\theta $, esto hace que nuestro trabajo sea mucho más fácil.\n",
    "\n",
    "El descenso de gradiente para la regresión logística se realiza nuevamente de la misma manera:\n",
    "\n",
    "<ul>\n",
    "<li> Haga una suposición inicial pero inteligente de los valores de los parámetros $ \\theta $. </li>\n",
    "<li> Siga iterando mientras el valor de la función de costo no cumpla con sus criterios:\n",
    "<ul>\n",
    "<li> Con los valores actuales de $ \\theta $, calcule el gradiente de la función de costo J ($ \\Delta \\theta = - \\alpha \\frac{d}{d \\theta} J (x) $). </ li>\n",
    "<li> Actualice los valores de los parámetros $ \\theta: = \\theta + \\alpha \\Delta \\theta $ </li>\n",
    "<li> Complete estos nuevos valores en la función de hipótesis y calcule nuevamente el valor de la función de costo; </li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "\n",
    "<h1 style=\"text-align: center;\"> 4. Implementación </h1>\n",
    "<br>\n",
    "Ahora que hemos visto la teoría, echemos un vistazo a un ejemplo de regresión logística. Empezaremos con el ejemplo de alumnos que aprueban o no un curso.\n",
    "\n",
    "Generemos algunos puntos de datos. Hay $ n = 300 $ estudiantes participando en el curso Machine Learning y si un estudiante $ i $ aprueba ($ y_i = 1 $) o no ($ y_i = 0 $) depende de dos variables;\n",
    "\n",
    "<ul>\n",
    "  <li> $ x_i ^ {(1)} $: cuántas horas ha estudiado el estudiante $ i $ para el examen. </li>\n",
    "  <li> $ x_i ^ {(2)} $: cuántas horas ha dormido el estudiante $ i $ el día antes del examen. </li>\n",
    "</ul>\n",
    "&nbsp;\n",
    "\n",
    "En nuestro ejemplo, los resultados son bastante binarios; todos los que hayan estudiado menos de 4 horas reprobaron el curso, así como todos aquellos cuyo tiempo de estudio + tiempo de sueño sea menor o igual a 13 horas ($ x_i ^ {(1)} + x_i ^ {(2)} \\leq 13 $)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def func2(x_i):\n",
    "    if x_i[1]+x_i[2] >= 13: \n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def generate_data2(no_points):\n",
    "    X = np.zeros(shape=(no_points, 3))\n",
    "    Y = np.zeros(shape=no_points)\n",
    "    for ii in range(no_points):\n",
    "        X[ii][0] = 1\n",
    "        X[ii][1] = random.random()*9+0.5\n",
    "        X[ii][2] = random.random()*9+0.5\n",
    "        Y[ii] = func2(X[ii])\n",
    "    return X, Y\n",
    "\n",
    "X, Y = generate_data2(300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados se ven así (los puntos verdes indican un pase y los puntos rojos un fallo)\n",
    "\n",
    "<img src=\"img/logistic_regression_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos una clase *LogisticRegression*, que establece los valores de la tasa de aprendizaje $ \\alpha $ y el número máximo de iteraciones *max_iter* en su inicialización. <br>\n",
    "\n",
    "Los valores de *X*, *Y* se establecen cuando estas matrices se pasan a la función \"train ()\", y luego se determinan los valores de no_examples, no_features y theta. <br>\n",
    "<br>\n",
    "También tenemos las funciones de hipótesis, costo y gradiente. <br>\n",
    "<br>\n",
    "El método de descenso de gradiente utiliza estos métodos para actualizar los valores de $\\theta $. <br>\n",
    "<br>\n",
    "El método \"train ()\", primero establece los valores de las matrices X, Y y theta, y luego llama al método gradient_descent. <br>\n",
    "<br>\n",
    "Una vez que se han determinado los valores de $ \\theta $ con el método de descenso de gradiente, el entrenamiento del clasificador está completo y podemos usarlo para clasificar nuevos ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression():\n",
    "    \"\"\"\n",
    "    Class for performing logistic regression.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate = 0.7, max_iter = 1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.theta = []\n",
    "        self.no_examples = 0\n",
    "        self.no_features = 0\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "        \n",
    "    def add_bias_col(self, X):\n",
    "        bias_col = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate([bias_col, X], axis=1)\n",
    "              \n",
    "    def hypothesis(self, X):\n",
    "        return 1 / (1 + np.exp(-1.0 * np.dot(X, self.theta)))\n",
    "\n",
    "    def cost_function(self):\n",
    "        \"\"\"\n",
    "        We will use the binary cross entropy as the cost function. https://en.wikipedia.org/wiki/Cross_entropy\n",
    "        \"\"\"\n",
    "        predicted_Y_values = self.hypothesis(self.X)\n",
    "        cost = (-1.0/self.no_examples) * np.sum(self.Y * np.log(predicted_Y_values) + (1 - self.Y) * (np.log(1-predicted_Y_values)))\n",
    "        return cost\n",
    "        \n",
    "    def gradient(self):\n",
    "        predicted_Y_values = self.hypothesis(self.X)\n",
    "        grad = (-1.0/self.no_examples) * np.dot((self.Y-predicted_Y_values), self.X)\n",
    "        return grad\n",
    "        \n",
    "    def gradient_descent(self):\n",
    "        for iter in range(1,self.max_iter):\n",
    "            cost = self.cost_function()\n",
    "            delta = self.gradient()\n",
    "            self.theta = self.theta - self.learning_rate * delta\n",
    "            if iter % 100 == 0: print(\"iteration %s : cost %s \" % (iter, cost))\n",
    "        \n",
    "    def train(self, X, Y):\n",
    "        self.X = self.add_bias_col(X)\n",
    "        self.Y = Y\n",
    "        self.no_examples, self.no_features = np.shape(X)\n",
    "        self.theta = np.ones(self.no_features + 1)\n",
    "        self.gradient_descent()\n",
    "  \n",
    "    def classify(self, X):\n",
    "        X = self.add_bias_col(X)\n",
    "        predicted_Y = self.hypothesis(X)\n",
    "        predicted_Y_binary = np.round(predicted_Y)\n",
    "        return predicted_Y_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 : cost 0.9983852320674571 \n",
      "iteration 200 : cost 0.30611739327840953 \n",
      "iteration 300 : cost 0.9238336140529533 \n",
      "iteration 400 : cost 0.11595692125751006 \n",
      "iteration 500 : cost 0.048300750162493404 \n",
      "iteration 600 : cost 0.047660033145153664 \n",
      "iteration 700 : cost 0.04704722195005992 \n",
      "iteration 800 : cost 0.04646027588223429 \n",
      "iteration 900 : cost 0.04589736034463832 \n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.train(X, Y)\n",
    "predicted_Y = lr.classify(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "       0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
       "       0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
       "       1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
       "       0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0.])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "       0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
       "       0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
       "       1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
       "       0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
       "       0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0.])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Ejemplo 2: conjunto de datos Iris </h2>\n",
    "\n",
    "Ahora que el concepto de regresión logística es un poco más claro, ¡clasifiquemos los datos del mundo real!\n",
    "\n",
    "\n",
    "Uno de los conjuntos de datos de clasificación más famosos es el conjunto de datos de flores de iris. Este conjunto de datos consta de tres clases, donde cada ejemplo tiene cuatro características numéricas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def all_positives(determined_Y, label):\n",
    "    return Counter(determined_Y)[label]\n",
    "\n",
    "def true_positives(determined_Y, real_Y, label):\n",
    "    true_positives = 0\n",
    "    for ii in range(0,len(determined_Y)):\n",
    "        if determined_Y[ii] == label and real_Y[ii] == label: \n",
    "            true_positives+=1\n",
    "    return true_positives\n",
    "\n",
    "def false_negatives(determined_Y, real_Y, label):\n",
    "    false_negatives = 0\n",
    "    for ii in range(0,len(determined_Y)):\n",
    "        if determined_Y[ii] != label and real_Y[ii] == label: \n",
    "            false_negatives+=1\n",
    "    return false_negatives\n",
    "\n",
    "def precision(determined_Y, real_Y, label):\n",
    "    if float(all_positives(determined_Y, label)) == 0: return 0\n",
    "    return true_positives(determined_Y, real_Y, label) / float(all_positives(determined_Y, label))\n",
    "\n",
    "def recall(determined_Y, real_Y, label):\n",
    "    denominator = float((true_positives(determined_Y, real_Y, label) + false_negatives(determined_Y, real_Y, label)))\n",
    "    if denominator == 0: return 0\n",
    "    return true_positives(determined_Y, real_Y, label) / denominator\n",
    "\n",
    "def f1_score(determined_Y, real_Y, label = 1):\n",
    "    p = precision(determined_Y, real_Y, label)\n",
    "    r = recall(determined_Y, real_Y, label)\n",
    "    if p + r == 0: return 0\n",
    "    f1 = 2 * (p * r) / (p + r)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Logistic Regression Classifier\n",
      "iteration 100 : cost 0.5562395040303484 \n",
      "iteration 200 : cost 0.0777521466317497 \n",
      "iteration 300 : cost 0.07644100327165747 \n",
      "iteration 400 : cost 0.07531038067548004 \n",
      "iteration 500 : cost 0.07432607845944914 \n",
      "iteration 600 : cost 0.07346176671669712 \n",
      "iteration 700 : cost 0.07269689872193913 \n",
      "iteration 800 : cost 0.0720152422024752 \n",
      "iteration 900 : cost 0.07140382813562421 \n",
      "trained\n",
      "F1-score on the test-set for class 1 is: 0.9600000000000001\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    " \n",
    "to_bin_y = { 1: { 'Iris-setosa': 1, 'Iris-versicolor': 0, 'Iris-virginica': 0 },\n",
    "             2: { 'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 0 },\n",
    "             3: { 'Iris-setosa': 0, 'Iris-versicolor': 0, 'Iris-virginica': 1 }\n",
    "             }\n",
    " \n",
    "#loading the dataset\n",
    "datafile = 'iris.data'\n",
    "df = pd.read_csv(datafile, header=None)\n",
    "df_train = df.sample(frac=0.7)\n",
    "df_test = df.loc[~df.index.isin(df_train.index)]\n",
    "X_train = df_train.values[:,0:4].astype(float)\n",
    "y_train = df_train.values[:,4]\n",
    "X_test = df_test.values[:,0:4].astype(float)\n",
    "y_test = df_test.values[:,4]\n",
    " \n",
    "Y_train = np.array([to_bin_y[3][x] for x in y_train])\n",
    "Y_test = np.array([to_bin_y[3][x] for x in y_test])\n",
    " \n",
    "print(\"training Logistic Regression Classifier\")\n",
    "lr = LogisticRegression()\n",
    "lr.train(X_train, Y_train)\n",
    "print(\"trained\")\n",
    "predicted_Y_test = lr.classify(X_test)\n",
    "f1 = f1_score(predicted_Y_test, Y_test, 1)\n",
    "print(\"F1-score on the test-set for class %s is: %s\" % (1, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede ver, nuestra clase simple LogisticRegression puede clasificar el conjunto de datos de Iris con una gran precisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usemos scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "print(iris[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X[:, 0][y == 0]* X[:, 1][y == 0], X[:, 1][y == 0]* X[:, 2][y == 0], 'r.', label='satosa')\n",
    "plt.plot(X[:, 0][y == 1]* X[:, 1][y == 1], X[:, 1][y == 1]* X[:, 2][y == 1], 'b.', label='versicolor')\n",
    "plt.plot(X[:, 0][y == 2]* X[:, 1][y == 2], X[:, 1][y == 2]* X[:, 2][y == 2], 'g.', label='virginica')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(C=1e5, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, max_iter=1000)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
